# Config for training ALAE on MammoGans at resolution 128x128

NAME: mammogans_blur
PPL_CELEBA_ADJUSTMENT: True
DATASET:
  PART_COUNT: 16
  SIZE: 16480
  SIZE_TEST: 16484 - 16480
  PATH: /home/vikki/Desktop/BTP/ALAE/data/datasets/mammogans_blur/tfrecords/mammogans_blur-r%02d.tfrecords.%03d
  PATH_TEST: /home/vikki/Desktop/BTP/ALAE/data/datasets/mammogans_blur-test/tfrecords/mammogans_blur-r%02d.tfrecords.%03d
  MAX_RESOLUTION_LEVEL: 7

  SAMPLES_PATH: /home/vikki/Desktop/BTP/ALAE/dataset_samples/mammogans
  STYLE_MIX_PATH: style_mixing/test_images/set_celeba
MODEL:
  LATENT_SPACE_SIZE: 256
  LAYER_COUNT: 6
  MAX_CHANNEL_COUNT: 256
  START_CHANNEL_COUNT: 64
  DLATENT_AVG_BETA: 0.995
  MAPPING_LAYERS: 8
OUTPUT_DIR: mammogans_blur_result
TRAIN:
  BASE_LEARNING_RATE: 0.002
  EPOCHS_PER_LOD: 10
  LEARNING_DECAY_RATE: 0.1
  LEARNING_DECAY_STEPS: []
  TRAIN_EPOCHS: 800
  #                    4       8       16       32       64       128        256       512       1024
  LOD_2_BATCH_8GPU: [512,    256,     128,      64,      32,       32,        32,       32,        32]
  LOD_2_BATCH_4GPU: [512,    256,     128,      64,      32,       32,        32,       32,        16]
  LOD_2_BATCH_2GPU: [128,    128,     128,      64,      32,       32,        16]
  LOD_2_BATCH_1GPU: [128,    128,     128,      64,      32,       16]

  LEARNING_RATES: [0.0015,  0.0015,   0.0015,   0.0015,  0.0015,   0.0015,     0.002,     0.003,    0.003]
